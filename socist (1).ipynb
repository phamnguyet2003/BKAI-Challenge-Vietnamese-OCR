{"metadata":{"colab":{"name":"Copy of Vietnamese_Handwritten_Hackathon_CRNN_Solution.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"accelerator":"GPU","language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Vietnamese Handwritten Recognition with CRNN model","metadata":{"id":"ImHu0JlkvPQe"}},{"cell_type":"markdown","source":"## Data Loader","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom torchvision.transforms import ToTensor, Resize, Compose\nimport torchvision.transforms.functional as F\nimport os\nimport numpy as np\nfrom PIL import Image\n\n\ntrain_folder_path = '/kaggle/input/handwritten-ocr/training_data/new_train' \ntest_folder_path = '/kaggle/input/handwritten-ocr/public_test_data/new_public_test'\nlabel_file_path = '/kaggle/input/handwriting/train_gt.txt'\nroot = '/kaggle/input/handwritten-ocr'\n\ndef encode_to_num(text, char_list):\n    encoded_label = []\n    for char in text:\n        encoded_label.append(char_list.index(char)+1)\n    return encoded_label\n\nclass OCRDataset(Dataset):\n    def __init__(self, root, max_label_len, train=True, transform=None):\n        self.max_label_len = max_label_len\n\n        self.train = train\n        self.transform = transform\n        if train:\n            dir = os.path.join(root, train_folder_path)\n            paths = os.listdir(dir)\n            image_files = [os.path.join(dir, path) for path in paths]\n            label_file = label_file_path\n        else:\n            dir = os.path.join(root, test_folder_path)\n            paths = os.listdir(dir)\n            image_files = [os.path.join(dir, path) for path in paths]\n        \n        self.images_path = image_files\n        if train:\n            self.labels = []\n            with open(label_file, encoding='utf-8') as f:\n                self.labels = [line.split()[1] for line in f.readlines()]\n            char_list= set()\n            for label in self.labels:\n                char_list.update(set(label))\n            self.char_list = sorted(char_list)\n            for i in range(len(self.labels)):\n                self.labels[i] = encode_to_num(self.labels[i], self.char_list)\n\n    def __len__(self):\n        return len(self.images_path)\n    def __getitem__(self, idx):      \n        image_path = self.images_path[idx]\n        image = Image.open(image_path).convert('L')\n        if self.transform:\n            image = self.transform(image)\n        if self.train:\n            label = self.labels[idx]\n            padded_label = np.squeeze(pad_sequences([label], maxlen=self.max_label_len, padding='post', value = 0))\n            return image, padded_label, len(label)\n        else:\n            return image","metadata":{"execution":{"iopub.status.busy":"2023-09-15T10:45:25.020324Z","iopub.execute_input":"2023-09-15T10:45:25.020924Z","iopub.status.idle":"2023-09-15T10:45:25.041636Z","shell.execute_reply.started":"2023-09-15T10:45:25.020882Z","shell.execute_reply":"2023-09-15T10:45:25.040269Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Build Model","metadata":{}},{"cell_type":"code","source":"pip install torchsummary","metadata":{"execution":{"iopub.status.busy":"2023-09-15T10:35:00.609538Z","iopub.execute_input":"2023-09-15T10:35:00.610004Z","iopub.status.idle":"2023-09-15T10:35:17.067414Z","shell.execute_reply.started":"2023-09-15T10:35:00.609972Z","shell.execute_reply":"2023-09-15T10:35:17.065889Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.init as init\nfrom torchsummary import summary\n\nclass CRNN(nn.Module):\n    def __init__(self, num_classes, drop_out_rate = 0.35):\n        super().__init__()\n\n        #CNN\n        self.conv1 = nn.Sequential(\n        nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding='same', bias=True),\n        nn.BatchNorm2d(num_features=64),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2, stride=(2,2))\n        )\n        self.conv2 = nn.Sequential(\n        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding='same', bias=True),\n        nn.BatchNorm2d(num_features=128),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=(1,2), stride=(1,2))\n        )\n        self.conv3 = nn.Sequential(\n        nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding='same', bias=True),\n        nn.BatchNorm2d(num_features=256),\n        nn.ReLU(),\n        )\n        self.conv4 = nn.Sequential(\n        nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding='same', bias=True),\n        nn.Dropout(drop_out_rate),\n        nn.BatchNorm2d(num_features=256),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=(1,2), stride=(1,2))\n        )\n        self.conv5 = nn.Sequential(\n        nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding='same', bias=True),\n        nn.BatchNorm2d(num_features=512),\n        nn.ReLU(),\n        )\n        self.conv6 = nn.Sequential(\n        nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding='same', bias=True),\n        nn.Dropout(drop_out_rate),\n        nn.BatchNorm2d(num_features=512),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=(1,2), stride=(1,2))     \n        )   \n        self.conv7 = nn.Sequential(\n        nn.Conv2d(in_channels=512, out_channels=512, kernel_size=2, padding='same', bias=True),\n        nn.Dropout(0.25),\n        nn.BatchNorm2d(num_features=512),\n        nn.ReLU()\n        )\n\n        self.fc1 = nn.Sequential(\n        nn.Linear(4096, 256),\n        nn.ReLU())\n\n        #RNN\n        self.rnn1 = nn.LSTM(input_size=256, hidden_size=128, bidirectional=True, batch_first=True)\n        self.rnn2 = nn.LSTM(input_size=256, hidden_size=256, bidirectional=True, batch_first=True)\n        #FC\n        self.fc2 = nn.Linear(512, num_classes)\n        #Softmax\n        self.softmax = nn.LogSoftmax(dim=2)\n        \n        #He/Kaming weight initialization\n        self._init_weights()\n\n    def _init_weights(self):\n        for module in self.modules():\n            if isinstance(module, (nn.Conv2d, nn.Linear)):\n                init.kaiming_uniform_(module.weight)\n                if module.bias is not None:\n                    init.zeros_(module.bias)\n\n    def forward(self,x):\n        x = self.conv1(x)\n        x = self.conv2(x)        \n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.conv5(x)\n        x = self.conv6(x)\n        x = self.conv7(x)\n        \n        # CNN to RNN\n        x = x.permute(2, 0, 1, 3)\n        x = x.reshape(32, 8, -1)\n        x = self.fc1(x)\n\n        x = self.rnn1(x)[0]\n        x = self.rnn2(x)[0]\n        x = self.fc2(x)\n        x = self.softmax(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-09-15T10:35:26.592734Z","iopub.execute_input":"2023-09-15T10:35:26.593371Z","iopub.status.idle":"2023-09-15T10:35:26.633494Z","shell.execute_reply.started":"2023-09-15T10:35:26.593309Z","shell.execute_reply":"2023-09-15T10:35:26.632091Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# input_data = torch.rand(8, 1, 64, 128)\n    \n# model = CRNN(num_classes=188).cuda()\n# if torch.cuda.is_available():\n#     input_data = input_data.cuda()\n# while True:\n#     result = model(input_data)\n#     print(result.shape)\n#     break","metadata":{"execution":{"iopub.status.busy":"2023-09-15T10:36:10.137839Z","iopub.execute_input":"2023-09-15T10:36:10.138284Z","iopub.status.idle":"2023-09-15T10:36:10.145101Z","shell.execute_reply.started":"2023-09-15T10:36:10.138248Z","shell.execute_reply":"2023-09-15T10:36:10.143479Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Train model","metadata":{}},{"cell_type":"code","source":"!pip install mltu","metadata":{"execution":{"iopub.status.busy":"2023-09-15T10:49:23.562519Z","iopub.execute_input":"2023-09-15T10:49:23.562931Z","iopub.status.idle":"2023-09-15T10:49:39.843509Z","shell.execute_reply.started":"2023-09-15T10:49:23.562896Z","shell.execute_reply":"2023-09-15T10:49:39.842141Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Collecting mltu\n  Downloading mltu-1.1.0-py3-none-any.whl (39 kB)\nRequirement already satisfied: PyYAML>=6.0 in /opt/conda/lib/python3.10/site-packages (from mltu) (6.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from mltu) (4.66.1)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from mltu) (2.0.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from mltu) (1.23.5)\nRequirement already satisfied: opencv-python in /opt/conda/lib/python3.10/site-packages (from mltu) (4.8.0.76)\nRequirement already satisfied: Pillow>=9.4.0 in /opt/conda/lib/python3.10/site-packages (from mltu) (9.5.0)\nCollecting onnxruntime>=1.15.0 (from mltu)\n  Downloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: librosa>=0.9.2 in /opt/conda/lib/python3.10/site-packages (from mltu) (0.10.1)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from mltu) (3.7.2)\nRequirement already satisfied: audioread>=2.1.9 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.9.2->mltu) (3.0.0)\nRequirement already satisfied: scipy>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.9.2->mltu) (1.11.2)\nRequirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.9.2->mltu) (1.2.2)\nRequirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.9.2->mltu) (1.3.2)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.9.2->mltu) (5.1.1)\nRequirement already satisfied: numba>=0.51.0 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.9.2->mltu) (0.57.1)\nRequirement already satisfied: soundfile>=0.12.1 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.9.2->mltu) (0.12.1)\nRequirement already satisfied: pooch>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.9.2->mltu) (1.7.0)\nRequirement already satisfied: soxr>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.9.2->mltu) (0.3.6)\nRequirement already satisfied: typing-extensions>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.9.2->mltu) (4.6.3)\nRequirement already satisfied: lazy-loader>=0.1 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.9.2->mltu) (0.2)\nRequirement already satisfied: msgpack>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.9.2->mltu) (1.0.5)\nCollecting coloredlogs (from onnxruntime>=1.15.0->mltu)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.15.0->mltu) (23.5.26)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.15.0->mltu) (21.3)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.15.0->mltu) (3.20.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.15.0->mltu) (1.12)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mltu) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mltu) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mltu) (4.40.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mltu) (1.4.4)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mltu) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mltu) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->mltu) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->mltu) (2023.3)\nRequirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.51.0->librosa>=0.9.2->mltu) (0.40.1)\nRequirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.0->librosa>=0.9.2->mltu) (3.10.0)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.0->librosa>=0.9.2->mltu) (2.31.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->mltu) (1.16.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->librosa>=0.9.2->mltu) (3.1.0)\nRequirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile>=0.12.1->librosa>=0.9.2->mltu) (1.15.1)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.15.0->mltu)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime>=1.15.0->mltu) (1.3.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa>=0.9.2->mltu) (2.21)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.9.2->mltu) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.9.2->mltu) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.9.2->mltu) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.9.2->mltu) (2023.7.22)\nInstalling collected packages: humanfriendly, coloredlogs, onnxruntime, mltu\nSuccessfully installed coloredlogs-15.0.1 humanfriendly-10.0 mltu-1.1.0 onnxruntime-1.15.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nfrom datetime import datetime\n\nfrom mltu.configs import BaseModelConfigs\n\nclass ModelConfigs(BaseModelConfigs):\n    def __init__(self):\n        super().__init__()\n        self.trained_models = 'trained_model'\n        self.root = 'data'\n        self.height = 64\n        self.width = 128\n        self.max_label_len = 16\n        self.epochs = 100        \n        self.batch_size = 8\n        self.learning_rate = 1e-3\n        self.train_workers = 4\n        self.logging = '/kaggle/input/handwriting/tensorboard/tensorboard'\n        self.checkpoint = None","metadata":{"execution":{"iopub.status.busy":"2023-09-15T10:49:44.993791Z","iopub.execute_input":"2023-09-15T10:49:44.994298Z","iopub.status.idle":"2023-09-15T10:49:45.308607Z","shell.execute_reply.started":"2023-09-15T10:49:44.994255Z","shell.execute_reply":"2023-09-15T10:49:45.307499Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# import torch.utils.data as data\n# from torchvision import datasets, transforms\n# import os\n# import torch\n# import torch.nn as nn\n# # from OCRDataset import OCRDataset\n# from torchvision.transforms import ToTensor, Resize, Compose, RandomAffine, ColorJitter\n# from torchvision.transforms import ToTensor, Resize, Compose\n# from torch.utils.data import DataLoader\n# from torch.utils.data import random_split\n# # from model import CRNN\n# import itertools\n# import numpy as np\n# from argparse import ArgumentParser\n# # from config import ModelConfigs\n# from tqdm.autonotebook import tqdm\n# from torch.utils.tensorboard import SummaryWriter\n# import shutil\n# import warnings\n# warnings.simplefilter(\"ignore\")\n\n# # augment data\n# augment_transform= Compose([RandomAffine(\n#                                             degrees=(-5, 5),\n#                                             scale=(0.5, 1.05), \n#                                             shear=10),\n#                                             ColorJitter(\n#                                                         brightness=0.5, \n#                                                         contrast=0.5,\n#                                                         saturation=0.5,\n#                                                         hue=0.5)\n#                            ])\n\n# def validation(model, device, val_dataloader, loss_function):\n#     model.eval()\n#     loss_value = 0\n#     for iter, (images, padded_labels, label_lenghts) in enumerate(val_dataloader):\n#         images = images.to(device)\n#         padded_labels = padded_labels.to(device)\n#         with torch.no_grad():\n#             predictions = model(images)  \n#             loss_value = criterion(predictions, padded_labels, output_lengths, label_lenghts)\n#     writer.add_scalar(\"Val/Loss\", loss_value, epoch)\n#     checkpoint = {\n#         \"epoch\": epoch + 1,\n#         \"best_loss\" : best_loss,\n#         \"model\": model.state_dict(),\n#         \"optimizer\": optimizer.state_dict()\n#     }\n#     torch.save(checkpoint, \"{}/last_crnn.pt\".format(trained_models))\n\n#     if loss_value >= best_loss:\n#         torch.save(checkpoint, \"{}/best_crnn.pt\".format(trained_models))\n#         best_loss = loss_value\n#     return loss_value\n\n# def traindata(device, model, start_epoch, epochs, optimizer, loss_function , train_loader, valid_loader):\n#     # Early stopping\n#     last_loss = 100\n#     patience = 2\n#     triggertimes = 0\n    \n#     for epoch in range(start_epoch, epochs):\n#         model.train()\n#         progress_bar = tqdm(train_loader, colour=\"green\")\n#         for iter, (images, padded_labels, label_lenghts) in enumerate(train_loader):\n#             images = augment_transform(images)\n#             images = images.to(device)\n#             padded_labels = padded_labels.to(device)\n#             #forward\n#             outputs = model(images)\n#             loss_value = loss_function(outputs, padded_labels, output_lengths, label_lenghts)\n#             if torch.isinf(loss_value):\n#                 print(outputs)\n#                 exit()\n#             progress_bar.set_description(\"Epoch {}/{}. Iteration {}/{}. Loss{:3f}\".format(epoch+1, num_epochs, iter+1, num_iters, loss_value))\n#             writer.add_scalar(\"Train/Loss\", loss_value, epoch*num_iters+iter)\n#             #backward\n#             optimizer.zero_grad()\n#             loss_value.backward()  \n#             optimizer.step()\n\n\n#             # Show progress\n#             if iter % 100 == 0 or iter == len(train_loader):\n#                 print('[{}/{}, {}/{}] loss: {:.8}'.format(epoch, epochs, iter, len(train_loader), loss_value.item()))\n\n#         # Early stopping\n#         current_loss = validation(model, device, valid_loader, loss_function)\n#         print('The Current Loss:', current_loss)\n\n#         if current_loss > last_loss:\n#             trigger_times += 1\n#             print('Trigger Times:', trigger_times)\n\n#             if trigger_times >= patience:\n#                 print('Early stopping!\\nStart to test process.')\n#                 return model\n\n#         else:\n#             print('trigger times: 0')\n#             trigger_times = 0\n\n#         last_loss = current_loss\n\n#     return model\n\n\n# def words_from_labels(labels, char_list):\n#     \"\"\"\n#     converts the list of encoded integer labels to word strings like eg. [12,10,29] returns CAT \n#     \"\"\"\n#     txt=[]\n#     for ele in labels:\n#         if ele == 0: # CTC blank space\n#             txt.append(\"\")\n#         else:\n#             #print(letters[ele])\n#             txt.append(char_list[ele+1])\n#     return \"\".join(txt)\n\n# def decode_batch(test_func, word_batch): #take only a sequence once a time\n#     \"\"\"\n#     Takes the Batch of Predictions and decodes the Predictions by Best Path Decoding and Returns the Output\n#     \"\"\"\n#     out = test_func([word_batch])[0] #returns the predicted output matrix of the model\n#     ret = []\n#     for j in range(out.shape[0]):\n#         out_best = list(np.argmax(out[j, :], 1))\n#         out_best = [k for k, g in itertools.groupby(out_best)]\n#         outstr = words_from_labels(out_best)\n#         ret.append(outstr)\n#     return ret\n\n# # device\n# if torch.cuda.is_available():\n#         device = torch.device(\"cuda\")\n# else:\n#     device = torch.device(\"cpu\")\n\n\n# configs = ModelConfigs()\n# root = configs.root\n# num_epochs = configs.epochs\n# batch_size = configs.batch_size\n# max_label_len = configs.max_label_len\n# height = configs.height\n# width = configs.width\n# learning_rate = configs.learning_rate\n# logging = configs.logging\n# trained_models = configs.trained_models\n# checkpoint = configs.checkpoint\n\n# transform = Compose([\n#         Resize((height,width)),\n#         ToTensor(),\n#          ])\n\n# #split train/val dataset\n# dataset = OCRDataset(root = train_folder_path, train=True, transform=transform)  # Replace with your dataset\n# train_size = int(0.9 * len(dataset))\n# val_size = len(dataset) - train_size\n# train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\n# train_dataloader = DataLoader(\n#     dataset=train_dataset,\n#     batch_size=batch_size,\n#     num_workers=4,\n#     drop_last=True,\n#     shuffle=True\n# )\n\n# val_dataloader = DataLoader(\n#     dataset=val_dataset,\n#     batch_size=batch_size,\n#     num_workers=4,\n#     drop_last=True,\n#     shuffle=True\n# )\n# # if not os.path.isdir(logging):\n# #     shutil.rmtree(logging)\n# # if not os.path.isdir(trained_models):\n# #     os.mkdir(trained_models)\n# writer = SummaryWriter(logging)\n# # Model architecture\n# # class CRNN(nn.Module):\n\n# char_list = dataset.char_list\n# model = CRNN(time_steps=max_label_len, num_classes=len(char_list)+1).to(device)\n# criterion = nn.CTCLoss(blank=0)\n# output_lengths = torch.full(size=(batch_size,), fill_value=max_label_len, dtype=torch.long)\n# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n# best_loss = 0\n# if checkpoint:\n#     checkpoint = torch.load(checkpoint)\n#     start_epoch = checkpoint['epoch']\n#     best_loss = checkpoint['best_loss']  \n#     model.load_state_dict(checkpoint[\"model\"])\n#     optimizer.load_state_dict(checkpoint[\"optimizer\"])\n# else:\n#     start_epoch = 0  \n# num_iters = len(train_dataloader)\n\n# traindata(device, model, start_epoch, num_epochs, optimizer, criterion , train_dataloader, val_dataloader)\n\n# validation(model, device, val_dataloader, criterion)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-15T09:40:19.371792Z","iopub.status.idle":"2023-09-15T09:40:19.372318Z","shell.execute_reply.started":"2023-09-15T09:40:19.372060Z","shell.execute_reply":"2023-09-15T09:40:19.372095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Add callbacks","metadata":{}},{"cell_type":"markdown","source":"## Save point --> Early Stopping --> ReduceLR","metadata":{}},{"cell_type":"code","source":"import torch\n\ndef save_checkpoint(model, optimizer, epoch, val_loss, checkpoint_path='checkpoint.pth'):\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'val_loss': val_loss,\n    }\n    torch.save(checkpoint, checkpoint_path)\n\n# Usage during training loop:\n# After evaluating validation loss\n# save_checkpoint(model, optimizer, epoch, val_loss, 'checkpoint.pth')\n\nclass EarlyStopping:\n    def __init__(self, patience=20, min_delta=1e-8, restore_best_weights=True):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.restore_best_weights = restore_best_weights\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n\n    def __call__(self, val_loss):\n        if self.best_loss is None:\n            self.best_loss = val_loss\n        elif val_loss < self.best_loss - self.min_delta:\n            self.best_loss = val_loss\n            self.counter = 0\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n\n# # Usage during training loop:\n# early_stopping = EarlyStopping(patience=20, min_delta=1e-8, restore_best_weights=True)\n# if early_stopping(val_loss):\n#     print(\"Early stopping triggered.\")\n#     if early_stopping.restore_best_weights:\n#         # Restore the model to the best state\n#         checkpoint = torch.load('checkpoint.pth')\n#         model.load_state_dict(checkpoint['model_state_dict'])\n#         optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\n\n# # Create a learning rate scheduler\n# scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=10, verbose=True)\n\n# # Usage during training loop:\n# scheduler.step(val_loss)  # Adjust learning rate based on validation loss","metadata":{"execution":{"iopub.status.busy":"2023-09-15T10:51:39.417465Z","iopub.execute_input":"2023-09-15T10:51:39.417877Z","iopub.status.idle":"2023-09-15T10:51:39.428990Z","shell.execute_reply.started":"2023-09-15T10:51:39.417846Z","shell.execute_reply":"2023-09-15T10:51:39.428058Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## ","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nfrom OCRDataset import OCRDataset\nfrom torchvision.transforms import ToTensor, Resize, Compose, RandomAffine, ColorJitter\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import random_split\nfrom model import CRNN\nimport itertools\nimport numpy as np\nfrom argparse import ArgumentParser\nfrom config import ModelConfigs\nfrom tqdm.autonotebook import tqdm\nfrom torch.utils.tensorboard import SummaryWriter\nimport shutil\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\ndef words_from_labels(labels, char_list):\n    \"\"\"\n    converts the list of encoded integer labels to word strings like eg. [12,10,29] returns CAT \n    \"\"\"\n    txt=[]\n    for ele in labels:\n        if ele == 0: # CTC blank space\n            txt.append(\"\")\n        else:\n            #print(letters[ele])\n            txt.append(char_list[ele+1])\n    return \"\".join(txt)\n\ndef decode_batch(test_func, word_batch): #take only a sequence once a time\n    \"\"\"\n    Takes the Batch of Predictions and decodes the Predictions by Best Path Decoding and Returns the Output\n    \"\"\"\n    out = test_func([word_batch])[0] #returns the predicted output matrix of the model\n    ret = []\n    for j in range(out.shape[0]):\n        out_best = list(np.argmax(out[j, :], 1))\n        out_best = [k for k, g in itertools.groupby(out_best)]\n        outstr = words_from_labels(out_best)\n        ret.append(outstr)\n    return ret\n\n\nif __name__ == '__main__':\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda\")\n    else:\n        device = torch.device(\"cpu\")\n\n    configs = ModelConfigs()\n    root = configs.root\n    num_epochs = configs.epochs\n    batch_size = configs.batch_size\n    train_workers = configs.train_workers\n    max_label_len = configs.max_label_len\n    height = configs.height\n    width = configs.width\n    learning_rate = configs.learning_rate\n    logging = configs.logging\n    trained_models = configs.trained_models\n    checkpoint = configs.checkpoint\n        \n    transform = Compose([\n        Resize((height,width)),\n        ToTensor(),\n         ])\n    \n    augment_transform= Compose([RandomAffine(\n                                            degrees=(-5, 5),\n                                            scale=(0.5, 1.05), \n                                            shear=10),\n                                ColorJitter(\n                                            brightness=0.5, \n                                            contrast=0.5,\n                                            saturation=0.5,\n                                            hue=0.5)])\n\n    #split train/val dataset\n    dataset = OCRDataset(root = root, max_label_len = max_label_len, train=True, transform=transform)\n    train_size = int(0.9 * len(dataset))\n    val_size = len(dataset) - train_size\n    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\n    train_dataloader = DataLoader(\n        dataset=train_dataset,\n        batch_size=batch_size,\n        num_workers=train_workers,\n        drop_last=True,\n        shuffle=True\n    )\n\n    val_dataloader = DataLoader(\n        dataset=val_dataset,\n        batch_size=batch_size,\n        num_workers=train_workers,\n        drop_last=True,\n        shuffle=True\n    )\n\n    if not os.path.isdir(logging):\n        shutil.rmtree(logging)\n    if not os.path.isdir(trained_models):\n        os.mkdir(trained_models)\n    writer = SummaryWriter(logging)\n\n    char_list = dataset.char_list\n    model = CRNN(num_classes=len(char_list)+1).to(device)\n    criterion = nn.CTCLoss(blank=0)\n    time_steps = max_label_len #time_steps(seq_len) must >= max_label_len but for simplicity, we use time_steps(seq_len) = max_label_len\n    output_lengths = torch.full(size=(batch_size,), fill_value=time_steps, dtype=torch.long)\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n    \n    # Initialize ReduceLROnPlateau scheduler\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n    # Early stopping \n    early_stopping = EarlyStopping(patience=20, min_delta=1e-8, restore_best_weights=True)\n    \n    best_loss = 0\n    early_stopping_counter = 0\n    early_stopping_patience = 10  # Adjust as needed\n\n    if checkpoint:\n        checkpoint = torch.load(checkpoint)\n        start_epoch = checkpoint['epoch']\n        best_loss = checkpoint['best_loss']  \n        model.load_state_dict(checkpoint[\"model\"])\n        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n    else:\n        start_epoch = 0  \n    num_iters = len(train_dataloader)\n\n    for epoch in range(start_epoch, num_epochs):\n        model.train()\n        progress_bar = tqdm(train_dataloader, colour=\"green\")\n        for iter, (images, padded_labels, label_lenghts) in enumerate(train_dataloader):\n            images = augment_transform(images)\n            images = images.to(device)\n            padded_labels = padded_labels.to(device)\n            #forward\n            outputs = model(images)\n#Shape:     #output(sequence_length, batch_size, num_classes)\n            #padded_labels(batch_size, max_label_len)\n            #output_lengths, label_lenghts(batch_size)\n            loss_value = criterion(outputs, padded_labels, output_lengths, label_lenghts)\n            if torch.isinf(loss_value):\n                print(outputs)\n                exit()\n            progress_bar.set_description(\"Epoch {}/{}. Iteration {}/{}. Loss{:3f}\".format(epoch+1, num_epochs, iter+1, num_iters, loss_value))\n            writer.add_scalar(\"Train/Loss\", loss_value, epoch*num_iters+iter)\n            #backward\n            optimizer.zero_grad()\n            loss_value.backward()  \n            optimizer.step()\n\n        model.eval()\n        for iter, (images, padded_labels, label_lenghts) in enumerate(val_dataloader):\n            images = images.to(device)\n            padded_labels = padded_labels.to(device)\n            with torch.no_grad():\n                predictions = model(images)  \n                loss_value = criterion(predictions, padded_labels, output_lengths, label_lenghts)\n        writer.add_scalar(\"Val/Loss\", loss_value, epoch)\n        # Update learning rate scheduler\n        scheduler.step(loss_value)\n        \n        checkpoint = {\n            \"epoch\": epoch + 1,\n            \"best_loss\" : best_loss,\n            \"model\": model.state_dict(),\n            \"optimizer\": optimizer.state_dict()\n        }\n        # torch.save(checkpoint, \"{}/last_crnn.pt\".format(trained_models))\n\n        # if loss_value >= best_loss:\n        #     torch.save(checkpoint, \"{}/best_crnn.pt\".format(trained_models))\n        #     best_loss = loss_value\n        # print('Validate', loss_value)\n        if early_stopping(val_loss):\n            print(\"Early stopping triggered.\")\n        if early_stopping.restore_best_weights:\n            checkpoint = torch.load('checkpoint.pth')\n            model.load_state_dict(checkpoint['model_state_dict'])\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            \n        print('Validate', loss_value)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-15T09:40:19.381475Z","iopub.status.idle":"2023-09-15T09:40:19.381940Z","shell.execute_reply.started":"2023-09-15T09:40:19.381704Z","shell.execute_reply":"2023-09-15T09:40:19.381727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nfrom OCRDataset import OCRDataset\nfrom torchvision.transforms import ToTensor, Resize, Compose, RandomAffine, ColorJitter\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import random_split\nfrom model import CRNN\nimport itertools\nimport numpy as np\nfrom argparse import ArgumentParser\nfrom config import ModelConfigs\nfrom tqdm.autonotebook import tqdm\nfrom torch.utils.tensorboard import SummaryWriter\nimport shutil\nimport warnings\nwarnings.simplefilter(\"ignore\")\nimport torch\n\ndef words_from_labels(labels, char_list):\n    \"\"\"\n    converts the list of encoded integer labels to word strings like eg. [12,10,29] returns CAT \n    \"\"\"\n    txt=[]\n    for ele in labels:\n        if ele == 0: # CTC blank space\n            txt.append(\"\")\n        else:\n            #print(letters[ele])\n            txt.append(char_list[ele+1])\n    return \"\".join(txt)\n\ndef decode_batch(test_func, word_batch): #take only a sequence once a time\n    \"\"\"\n    Takes the Batch of Predictions and decodes the Predictions by Best Path Decoding and Returns the Output\n    \"\"\"\n    out = test_func([word_batch])[0] #returns the predicted output matrix of the model\n    ret = []\n    for j in range(out.shape[0]):\n        out_best = list(np.argmax(out[j, :], 1))\n        out_best = [k for k, g in itertools.groupby(out_best)]\n        outstr = words_from_labels(out_best)\n        ret.append(outstr)\n    return ret\n\n\nif __name__ == '__main__':\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda\")\n    else:\n        device = torch.device(\"cpu\")\n\n    configs = ModelConfigs()\n    root = configs.root\n    num_epochs = configs.epochs\n    batch_size = configs.batch_size\n    train_workers = configs.train_workers\n    max_label_len = configs.max_label_len\n    height = configs.height\n    width = configs.width\n    learning_rate = configs.learning_rate\n    logging = configs.logging\n    trained_models = configs.trained_models\n    checkpoint = configs.checkpoint\n        \n    transform = Compose([\n        Resize((height,width)),\n        ToTensor(),\n         ])\n    \n    augment_transform= Compose([RandomAffine(\n                                            degrees=(-5, 5),\n                                            scale=(0.5, 1.05), \n                                            shear=10),\n                                ColorJitter(\n                                            brightness=0.5, \n                                            contrast=0.5,\n                                            saturation=0.5,\n                                            hue=0.5)])\n\n    #split train/val dataset\n    dataset = OCRDataset(root = root, max_label_len = max_label_len, train=True, transform=transform)\n    train_size = int(0.9 * len(dataset))\n    val_size = len(dataset) - train_size\n    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\n    train_dataloader = DataLoader(\n        dataset=train_dataset,\n        batch_size=batch_size,\n        num_workers=train_workers,\n        drop_last=True,\n        shuffle=True\n    )\n\n    val_dataloader = DataLoader(\n        dataset=val_dataset,\n        batch_size=batch_size,\n        num_workers=train_workers,\n        drop_last=True,\n        shuffle=True\n    )\n\n    if not os.path.isdir(logging):\n        shutil.rmtree(logging)\n    if not os.path.isdir(trained_models):\n        os.mkdir(trained_models)\n    writer = SummaryWriter(logging)\n\n    char_list = dataset.char_list\n    model = CRNN(num_classes=len(char_list)+1).to(device)\n    criterion = nn.CTCLoss(blank=0)\n    time_steps = max_label_len #time_steps(seq_len) must >= max_label_len but for simplicity, we use time_steps(seq_len) = max_label_len\n    output_lengths = torch.full(size=(batch_size,), fill_value=time_steps, dtype=torch.long)\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n    \n    # Initialize ReduceLROnPlateau scheduler\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n    # Early stopping \n    early_stopping = EarlyStopping(patience=20, min_delta=1e-8, restore_best_weights=True)\n    \n    best_loss = 0\n    early_stopping_counter = 0\n    early_stopping_patience = 10  # Adjust as needed\n\n    if checkpoint:\n        checkpoint = torch.load(checkpoint)\n        start_epoch = checkpoint['epoch']\n        best_loss = checkpoint['best_loss']  \n        model.load_state_dict(checkpoint[\"model\"])\n        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n    else:\n        start_epoch = 0  \n    num_iters = len(train_dataloader)\n\n    for epoch in range(start_epoch, num_epochs):\n        model.train()\n        progress_bar = tqdm(train_dataloader, colour=\"green\")\n        for iter, (images, padded_labels, label_lenghts) in enumerate(train_dataloader):\n            images = augment_transform(images)\n            images = images.to(device)\n            padded_labels = padded_labels.to(device)\n            #forward\n            outputs = model(images)\n#Shape:     #output(sequence_length, batch_size, num_classes)\n            #padded_labels(batch_size, max_label_len)\n            #output_lengths, label_lenghts(batch_size)\n            loss_value = criterion(outputs, padded_labels, output_lengths, label_lenghts)\n            if torch.isinf(loss_value):\n                print(outputs)\n                exit()\n            progress_bar.set_description(\"Epoch {}/{}. Iteration {}/{}. Loss{:3f}\".format(epoch+1, num_epochs, iter+1, num_iters, loss_value))\n            writer.add_scalar(\"Train/Loss\", loss_value, epoch*num_iters+iter)\n            #backward\n            optimizer.zero_grad()\n            loss_value.backward()  \n            optimizer.step()\n\n        model.eval()\n        for iter, (images, padded_labels, label_lenghts) in enumerate(val_dataloader):\n            images = images.to(device)\n            padded_labels = padded_labels.to(device)\n            with torch.no_grad():\n                predictions = model(images)  \n                loss_value = criterion(predictions, padded_labels, output_lengths, label_lenghts)\n        writer.add_scalar(\"Val/Loss\", loss_value, epoch)\n        # Update learning rate scheduler\n        scheduler.step(loss_value)\n        \n        checkpoint = {\n            \"epoch\": epoch + 1,\n            \"best_loss\" : best_loss,\n            \"model\": model.state_dict(),\n            \"optimizer\": optimizer.state_dict()\n        }\n        # torch.save(checkpoint, \"{}/last_crnn.pt\".format(trained_models))\n\n        # if loss_value >= best_loss:\n        #     torch.save(checkpoint, \"{}/best_crnn.pt\".format(trained_models))\n        #     best_loss = loss_value\n        # print('Validate', loss_value)\n        if early_stopping(val_loss):\n            print(\"Early stopping triggered.\")\n        if early_stopping.restore_best_weights:\n            checkpoint = torch.load('checkpoint.pth')\n            model.load_state_dict(checkpoint['model_state_dict'])\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            \n        print('Validate', loss_value)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-15T09:40:19.391557Z","iopub.status.idle":"2023-09-15T09:40:19.392100Z","shell.execute_reply.started":"2023-09-15T09:40:19.391799Z","shell.execute_reply":"2023-09-15T09:40:19.391823Z"},"trusted":true},"execution_count":null,"outputs":[]}]}